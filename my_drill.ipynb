{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95dff57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aid                                    content_Article id           law_id\n",
      "0    0  1. Thông tư này quy định mã số, tiêu chuẩn chu...  0  14/2022/TT-NHNN\n",
      "1    1  1. Kiểm soát viên cao cấp ngân hàng Mã số: 07....  0  14/2022/TT-NHNN\n",
      "2    2  1. Có bản lĩnh chính trị vững vàng, kiên định ...  0  14/2022/TT-NHNN\n",
      "3    3  1. Chức trách Là công chức có trình độ chuyên ...  0  14/2022/TT-NHNN\n",
      "4    4  1. Chức trách Là công chức có trình độ chuyên ...  0  14/2022/TT-NHNN\n",
      "     qid                                           question  \\\n",
      "0    933  Thưa luật sư tôi có đăng ký kết hôn trên pháp ...   \n",
      "1   2997  Ai có quyền điều hành hoạt động của liên hiệp ...   \n",
      "2  12282  Trình tự đăng ký hành nghề dịch vụ kế toán đượ...   \n",
      "3    340  Thời hạn giải quyết hồ sơ nhận con nuôi là bao...   \n",
      "4   7418  Hoạt động chuyển giao công nghệ trong cơ sở gi...   \n",
      "\n",
      "           relevant_laws  \n",
      "0  [53877, 53875, 53929]  \n",
      "1                [24221]  \n",
      "2  [29515, 29512, 58071]  \n",
      "3                 [3543]  \n",
      "4                [44548]  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        law_docs_iterator = json.load(f)\n",
    "        return law_docs_iterator\n",
    "\n",
    "\n",
    "\n",
    "def legal_corpus_to_dataframe():\n",
    "    \"\"\"\n",
    "    Chuyển đổi corpus pháp lý thành DataFrame.\n",
    "    \"\"\"\n",
    "    read_legal_corpus = load_jsonl('data/legal_corpus.json')\n",
    "    legal_df = pd.json_normalize(\n",
    "        read_legal_corpus,\n",
    "        meta=['id', 'law_id'],\n",
    "        record_path='content'\n",
    "    )\n",
    "    return legal_df\n",
    "\n",
    "\n",
    "df_legal_corpus = legal_corpus_to_dataframe()\n",
    "print(df_legal_corpus.head())\n",
    "df_train =  pd.DataFrame(load_jsonl('data/train.json'))\n",
    "print(df_train.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db97386",
   "metadata": {},
   "source": [
    "## Hàm Tiền Xử Lý Văn Bản Chung (`preprocess_text`)\n",
    "\n",
    "Hàm này thực hiện các bước tiền xử lý văn bản như sau:\n",
    "\n",
    "1. **Chuyển về chữ thường:** Đồng nhất các từ để tránh phân biệt hoa/thường.\n",
    "2. **Xử lý ký tự xuống dòng:** Thay thế ký tự `\\n` bằng khoảng trắng để nối các dòng lại với nhau.\n",
    "3. **Chuẩn hóa khoảng trắng:** Loại bỏ các khoảng trắng thừa, chỉ giữ lại một khoảng trắng giữa các từ.\n",
    "4. **Tách từ (Word Segmentation):** Sử dụng `underthesea.word_tokenize` để tách từ tiếng Việt. Các từ ghép sẽ được nối bằng dấu gạch dưới (ví dụ: `kinh_doanh`).\n",
    "5. **Loại bỏ các ký tự không phải chữ cái, số, hoặc dấu gạch dưới:** Giữ lại các từ đã tách và các số, loại bỏ các ký tự đặc biệt khác.\n",
    "6. **Thêm dấu cách cho dấu câu:** Đảm bảo dấu câu được tách riêng, không dính vào từ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414f1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from underthesea import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Tiền xử lý văn bản:\n",
    "    - Chuyển về chữ thường\n",
    "    - Xử lý ký tự xuống dòng\n",
    "    - Chuẩn hóa khoảng trắng\n",
    "    - Tách từ tiếng Việt\n",
    "    - Loại bỏ ký tự đặc biệt không cần thiết\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # 1. Chuyển về chữ thường\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Xử lý ký tự xuống dòng và chuẩn hóa khoảng trắng\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # 3. Thêm khoảng trắng quanh dấu câu để đảm bảo tách từ đúng\n",
    "    # (Giữ lại các dấu cơ bản, loại bỏ các ký tự đặc biệt khác)\n",
    "    text = re.sub(r'([.,!?;:()])', r' \\1 ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()    \n",
    "\n",
    "    # 4. Tách từ bằng underthesea\n",
    "    # `word_tokenize` trả về danh sách các từ đã tách\n",
    "    tokens = word_tokenize(text, format=\"text\") # format=\"text\" trả về chuỗi đã nối gạch dưới\n",
    "\n",
    "    # 5. Loại bỏ các ký tự không phải chữ cái, số, hoặc dấu gạch dưới\n",
    "    # Giữ lại các từ đã tách và các số, bỏ các ký tự đặc biệt khác\n",
    "    text_processed = re.sub(r'[^\\p{L}\\p{N}_ ]+', '', tokens, flags=re.UNICODE)\n",
    "    text_processed = re.sub(r'\\s+', ' ', text_processed).strip()\n",
    "\n",
    "    return text_processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803c51a",
   "metadata": {},
   "source": [
    "**Tiền Xử Lý Dữ Liệu Huấn Luyện (train_data.json)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c654778f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     qid                                           question  \\\n",
      "0    933  Thưa luật sư tôi có đăng ký kết hôn trên pháp ...   \n",
      "1   2997  Ai có quyền điều hành hoạt động của liên hiệp ...   \n",
      "2  12282  Trình tự đăng ký hành nghề dịch vụ kế toán đượ...   \n",
      "3    340  Thời hạn giải quyết hồ sơ nhận con nuôi là bao...   \n",
      "4   7418  Hoạt động chuyển giao công nghệ trong cơ sở gi...   \n",
      "\n",
      "           relevant_laws                                       pre_question  \n",
      "0  [53877, 53875, 53929]  thưa luật_sư tôi có đăng_ký kết_hôn trên pháp_...  \n",
      "1                [24221]  ai có quyền điều_hành hoạt_động của liên_hiệp ...  \n",
      "2  [29515, 29512, 58071]  trình_tự đăng_ký hành_nghề dịch_vụ kế_toán đượ...  \n",
      "3                 [3543]  thời_hạn giải_quyết hồ_sơ nhận con_nuôi là bao...  \n",
      "4                [44548]  hoạt_động chuyển_giao công_nghệ trong cơ_sở gi...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kat/KAT/TTNT/NLP/nlp-drill-chatbot/venv/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:4399: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('question', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "df_train = dd.from_pandas(df_train, npartitions=4)\n",
    "df_train['pre_question'] = df_train['question'].apply(\n",
    "   lambda x: preprocess_text(x)\n",
    ")\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06cd452",
   "metadata": {},
   "source": [
    "**Tiền Xử Lý Kho Văn Bản Pháp Luật (legal_corpus.json) và Xây Dựng Cấu Trúc Tra Cứu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0116a528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kat/KAT/TTNT/NLP/nlp-drill-chatbot/venv/lib/python3.11/site-packages/dask/dataframe/dask_expr/_collection.py:4399: UserWarning: \n",
      "You did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\n",
      "To provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('content_Article', 'object'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         aid                                    content_Article    id  \\\n",
      "59631  59631  Các Bộ trưởng, Thủ trưởng cơ quan ngang bộ, Th...  2154   \n",
      "59632  59632  1. Sửa\\r\\nđổi, bổ sung khoản 2 Điều 3\\nnhư sau...  2155   \n",
      "59633  59633  1. Bộ trưởng Bộ Công Thương chịu trách nhiệm t...  2155   \n",
      "59634  59634  1. Nghị định này có hiệu lực thi hành từ ngày ...  2155   \n",
      "59635  59635  1. Sửa đổi\\nkhoản 3 Điều 9 như sau: “3. Học ph...  2156   \n",
      "\n",
      "              law_id                                        pre_content  \n",
      "59631  72/2019/NĐ-CP  các bộ_trưởng thủ_trưởng cơ_quan ngang bộ thủ_...  \n",
      "59632  18/2023/NĐ-CP  1 sửa_đổi bổ_sung khoản 2 điều 3 như sau 2 doa...  \n",
      "59633  18/2023/NĐ-CP  1 bộ_trưởng bộ công_thương chịu trách_nhiệm tổ...  \n",
      "59634  18/2023/NĐ-CP  1 nghị_định này có hiệu_lực thi_hành từ ngày 2...  \n",
      "59635  97/2023/NĐ-CP  1 sửa_đổi khoản 3 điều 9 như sau 3 học_phí từ ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_legal_articles = dd.from_pandas(df_legal_corpus, npartitions=4)\n",
    "df_legal_articles['pre_content'] = df_legal_articles['content_Article'].apply(\n",
    "   lambda x: preprocess_text(x)\n",
    ")\n",
    "print(df_legal_articles.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8b4cc",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Kiến trúc (Architecture)\n",
    "    - Encoder A (Query Encoder): Chuyên trách biến đổi các câu truy vấn (query) hoặc câu hỏi thành một vector số (embedding).\n",
    "\n",
    "    - Encoder B (Document/Passage Encoder): Chuyên trách biến đổi các tài liệu (document), đoạn văn bản (passage), hoặc điều luật thành một vector số (embedding).\n",
    "\n",
    "## 2. Giai đoạn Huấn luyện (Training Phase)**\n",
    "    - Mục tiêu của huấn luyện Bi-encoder là điều chỉnh các trọng số của hai encoder (thực chất là một encoder duy nhất chia sẻ trọng số) sao cho:\n",
    "\n",
    "    - Vector nhúng của một câu hỏi liên quan gần với vector nhúng của tài liệu trả lời nó.\n",
    "\n",
    "    - Vector nhúng của một câu hỏi không liên quan xa với vector nhúng của tài liệu không liên quan.\n",
    "\n",
    "    - Quá trình này thường sử dụng kỹ thuật Học tương phản (Contrastive Learning).\n",
    "**Các bước huấn luyện:**\n",
    "- **Đầu vào huấn luyện: Mô hình nhận vào các bộ ba (triplets) hoặc cặp (pairs):**\n",
    "\n",
    "    - (Query Q, Positive Document D+): Một câu hỏi và một tài liệu thực sự liên quan đến câu hỏi đó.\n",
    "\n",
    "    - (Query Q, Negative Document D-): Một câu hỏi và một tài liệu không liên quan đến câu hỏi đó. Tài liệu tiêu cực có thể được lấy mẫu ngẫu nhiên hoặc thông qua các chiến lược lấy mẫu \"hard negative\" (ví dụ: lấy tài liệu mà mô hình ban đầu nhầm lẫn là liên quan).\n",
    "\n",
    "- **Quá trình mã hóa:**\n",
    "\n",
    "    - Câu hỏi Q đi qua Encoder A để tạo ra Q_emb.\n",
    "\n",
    "    - Tài liệu tích cực D+ đi qua Encoder B để tạo ra D_+emb.\n",
    "\n",
    "    - Tài liệu tiêu cực D- đi qua Encoder B để tạo ra D_−emb.\n",
    "\n",
    "- **Tính toán Độ tương đồng:**\n",
    "    - Mô hình tính toán độ tương đồng giữa Q_emb và D_+emb (Positive Similarity).\n",
    "\n",
    "    - Mô hình tính toán độ tương đồng giữa Q_emb và D_−emb (Negative Similarity).\n",
    "\n",
    "    - Phép tính độ tương đồng phổ biến nhất là độ tương đồng cosin (cosine similarity) hoặc tích vô hướng (dot product).\n",
    "\n",
    "- **Hàm mất mát (Loss Function) - Ví dụ: Multiple Negative Ranking Loss (MNRL):**\n",
    "\n",
    "    - MNRL (được dùng trong thư viện sentence-transformers) là một loại Contrastive Loss hiệu quả.\n",
    "\n",
    "    - Trong một batch huấn luyện, nó xem xét mỗi câu hỏi và tài liệu tích cực tương ứng.\n",
    "\n",
    "    - Nó coi tất cả các tài liệu khác trong cùng batch (không phải là tài liệu tích cực của câu hỏi hiện tại) là các tài liệu tiêu cực tiềm năng.\n",
    "\n",
    "    - Hàm mất mát sẽ cố gắng tối đa hóa điểm số của cặp (Q, D+) và tối thiểu hóa điểm số của cặp (Q, D-) trong cùng batch. Mục tiêu là làm cho Positive Similarity cao hơn Negative Similarity một khoảng an toàn (margin).\n",
    "\n",
    "    - Về cơ bản, nó muốn xác suất của cặp (Q, D+) cao hơn các cặp (Q, D_khác).\n",
    "\n",
    "- **Cập nhật trọng số:**\n",
    "    - Dựa trên giá trị của hàm mất mát, thuật toán tối ưu hóa (ví dụ: AdamW) sẽ điều chỉnh trọng số của các encoder thông qua lan truyền ngược (backpropagation) để cải thiện khả năng phân biệt giữa các cặp liên quan và không liên quan.\n",
    "\n",
    "## 3. Giai đoạn Sử dụng / Suy luận (Inference/Deployment Phase)\n",
    "Sau khi Bi-encoder được huấn luyện xong, nó đã học được cách tạo ra các vector nhúng có ý nghĩa. Bây giờ chúng ta sử dụng nó để tìm kiếm:\n",
    "\n",
    "- **Mã hóa Corpus Tài liệu (Offline Pre-computation):**\n",
    "\n",
    "    - Sử dụng Encoder B để mã hóa tất cả các tài liệu (ví dụ: hàng ngàn, hàng triệu điều luật hoặc chunks điều luật) thành các vector nhúng.\n",
    "\n",
    "    - Quá trình này chỉ cần thực hiện một lần và có thể mất nhiều thời gian nếu corpus lớn.\n",
    "\n",
    "    - Các vector nhúng này được lưu trữ trong một chỉ mục tìm kiếm hiệu quả như FAISS. FAISS được tối ưu hóa để tìm kiếm láng giềng gần nhất (Nearest Neighbor Search) trong không gian vector đa chiều một cách rất nhanh chóng.\n",
    "\n",
    "- **Xử lý Câu hỏi mới (Online Query):**\n",
    "    - Khi một người dùng nhập một câu hỏi mới, câu hỏi đó sẽ được đưa qua Encoder A để tạo ra vector nhúng của câu hỏi đó (Q_new_emb).\n",
    "\n",
    "    - Vector Q_new_emb này sau đó được sử dụng để truy vấn chỉ mục FAISS.\n",
    "\n",
    "    - FAISS sẽ nhanh chóng tìm và trả về K vector tài liệu gần nhất với Q_new_emb trong không gian vector.\n",
    "\n",
    "    - FAISS cũng trả về chỉ mục của các vector này, cho phép chúng ta tra cứu lại các tài liệu gốc tương ứng.\n",
    "\n",
    "\n",
    "## GIAI ĐOẠN HUẤN LUYỆN:\n",
    "\n",
    "    Câu hỏi Q   ------------> [Encoder A] -----> Q_emb\n",
    "                                                 |\n",
    "                                                 | (So sánh độ tương đồng và tính Loss)\n",
    "                                                 |\n",
    "    Tài liệu D+ ------------> [Encoder B] -----> D+_emb\n",
    "    Tài liệu D- ------------> [Encoder B] -----> D-_emb\n",
    "                                                    |\n",
    "                                                    V\n",
    "                                            Hàm mất mát (MNRL/Triplet)\n",
    "                                            (Điều chỉnh trọng số Encoder A & B)\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "## GIAI ĐOẠN SỬ DỤNG (Tìm kiếm):\n",
    "\n",
    "    1. Mã hóa Corpus (làm 1 lần):\n",
    "    Tài liệu D1 ---------> [Encoder B] -----> D1_emb\n",
    "    Tài liệu D2 ---------> [Encoder B] -----> D2_emb\n",
    "    ...\n",
    "    Tài liệu DN ---------> [Encoder B] -----> DN_emb\n",
    "                            (Tất cả D_emb được lưu vào FAISS Index)\n",
    "\n",
    "    2. Xử lý Query mới (mỗi lần có câu hỏi):\n",
    "    Câu hỏi Q_new ------> [Encoder A] -----> Q_new_emb\n",
    "                                                    |\n",
    "                                                    V\n",
    "                                                [FAISS Index] (Tìm kiếm các D_emb gần nhất)\n",
    "                                                    |\n",
    "                                                    V\n",
    "                                        Các Tài liệu liên quan nhất"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f199ca57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kat/KAT/TTNT/NLP/nlp-drill-chatbot/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class BiEncoderDatasetFromDF(Dataset):\n",
    "    def __init__(self, df_train_data: pd.DataFrame, df_corpus_data: pd.DataFrame):\n",
    "        self.df_train = df_train_data\n",
    "        self.df_corpus = df_corpus_data\n",
    "        \n",
    "        self.data_samples = []\n",
    "        \n",
    "        # Iterate through each question in df_train\n",
    "        for index, row in tqdm(self.df_train.iterrows(), total=len(self.df_train), desc=\"Chuẩn bị cặp huấn luyện\"):\n",
    "            question_text = row['pre_question'] \n",
    "            relevant_aids = row['relevant_laws']\n",
    "            \n",
    "            for aid_in_relevant_laws in relevant_aids:\n",
    "                query_result = self.df_corpus.query(f\"aid == {aid_in_relevant_laws}\")\n",
    "                \n",
    "                positive_chunk_content = None\n",
    "                if not query_result.empty:\n",
    "                    # Lấy nội dung từ cột 'pre_content' và phần tử đầu tiên của mảng values\n",
    "                    positive_chunk_content = query_result['pre_content'].values[0]\n",
    "                \n",
    "                if positive_chunk_content is not None:\n",
    "                    self.data_samples.append(InputExample(texts=[question_text, positive_chunk_content]))\n",
    "                # else:\n",
    "                    # print(f\"Cảnh báo: AID {aid_in_relevant_laws} từ relevant_laws không tìm thấy trong df_legal_articles.\")\n",
    "\n",
    "        print(f\"Đã tạo {len(self.data_samples)} cặp huấn luyện (câu hỏi, chunk điều luật tích cực).\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_samples[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
