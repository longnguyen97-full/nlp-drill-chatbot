# üèõÔ∏è LawBot - Legal QA Pipeline v8.1 (State-of-the-Art Optimized)

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-8.0-green.svg)](https://github.com/lawbot-team/lawbot)
[![Pipeline](https://img.shields.io/badge/pipeline-cascaded--reranking-orange.svg)](https://github.com/lawbot-team/lawbot)

> **H·ªá th·ªëng h·ªèi-ƒë√°p ph√°p lu·∫≠t th√¥ng minh cho Vi·ªát Nam**  
> **Phi√™n b·∫£n v8.1: T·ªëi ∆∞u h√≥a to√†n di·ªán v√† hi·ªáu su·∫•t cao nh·∫•t**  
> T√≠ch h·ª£p c√°c k·ªπ thu·∫≠t AI ti√™n ti·∫øn nh·∫•t v√† t·ªëi ∆∞u h√≥a tri·ªát ƒë·ªÉ

## ‚ú® **T√çNH NƒÇNG N·ªîI B·∫¨T (KEY FEATURES)**

Phi√™n b·∫£n v8.1 t√≠ch h·ª£p c√°c k·ªπ thu·∫≠t h√†ng ƒë·∫ßu v√† t·ªëi ∆∞u h√≥a tri·ªát ƒë·ªÉ:

- **Ki·∫øn tr√∫c X·∫øp h·∫°ng ƒêa t·∫ßng (Cascaded Reranking)**: M·ªôt "ph·ªÖu l·ªçc" 3 t·∫ßng th√¥ng minh gi√∫p c√¢n b·∫±ng ho√†n h·∫£o gi·ªØa t·ªëc ƒë·ªô v√† ƒë·ªô ch√≠nh x√°c, cho ph√©p h·ªá th·ªëng x·ª≠ l√Ω hi·ªáu qu·∫£ m·ªôt l∆∞·ª£ng l·ªõn th√¥ng tin.

- **Domain-Adaptive Pre-training (DAPT)**: Kh·∫£ nƒÉng "chuy√™n m√¥n h√≥a" model ng√¥n ng·ªØ, bi·∫øn PhoBERT t·ª´ m·ªôt chuy√™n gia ƒëa ng√†nh th√†nh m·ªôt chuy√™n gia am hi·ªÉu s√¢u s·∫Øc v·ªÅ ph√°p lu·∫≠t (PhoBERT-Law).

- **H·ªôi ƒë·ªìng Chuy√™n gia (Ensemble Reranking)**: S·ª≠ d·ª•ng nhi·ªÅu model Cross-Encoder c√πng th·∫©m ƒë·ªãnh k·∫øt qu·∫£, gi√∫p tƒÉng c∆∞·ªùng s·ª± ·ªïn ƒë·ªãnh v√† ƒë·ªô tin c·∫≠y cho c√¢u tr·∫£ l·ªùi cu·ªëi c√πng.

- **Khai th√°c Hard Negatives T·ª± ƒë·ªông**: T·ª± ƒë·ªông "ƒë√†o" ra nh·ªØng v√≠ d·ª• h·ªçc kh√≥ nh·∫•t, bu·ªôc AI ph·∫£i h·ªçc c√°ch ph√¢n bi·ªát nh·ªØng kh√°c bi·ªát ng·ªØ nghƒ©a tinh vi trong vƒÉn b·∫£n lu·∫≠t.

- **T·ªëi ∆∞u h√≥a Hi·ªáu su·∫•t V·∫≠n h√†nh (v8.1)**: 
  - **GPU Acceleration**: T·ª± ƒë·ªông detect v√† s·ª≠ d·ª•ng GPU v·ªõi mixed precision (FP16)
  - **Optimized Model Loading**: T√°i s·ª≠ d·ª•ng model c√≥ s·∫µn thay v√¨ train t·∫°m th·ªùi
  - **Single Data Loading**: T·∫£i d·ªØ li·ªáu m·ªôt l·∫ßn duy nh·∫•t cho to√†n b·ªô pipeline
  - **Memory Management**: T·ªëi ∆∞u h√≥a memory usage v√† cleanup t·ª± ƒë·ªông

- **C·∫•u tr√∫c Code T·ªëi ∆∞u (v8.1)**: 
  - **Unified Utils Package**: T·ªï ch·ª©c l·∫°i th√†nh `core/utils/` v·ªõi c√°c module chuy√™n bi·ªát
  - **Centralized Configuration**: T·∫•t c·∫£ "magic numbers" ƒë∆∞·ª£c chuy·ªÉn v√†o `config.py`
  - **Clean Naming**: Lo·∫°i b·ªè tr√πng l·∫∑p v√† ƒë·∫∑t t√™n r√µ r√†ng h∆°n
  - **Error Handling**: X·ª≠ l√Ω l·ªói tri·ªát ƒë·ªÉ v·ªõi fallback mechanisms
  - **Logging System**: H·ªá th·ªëng logging chi ti·∫øt v√† monitoring

- **Pipeline T·ªëi ∆∞u & B·ªÅn b·ªâ**: To√†n b·ªô quy tr√¨nh ƒë∆∞·ª£c ƒë√≥ng g√≥i th√†nh c√°c b∆∞·ªõc logic, d·ªÖ qu·∫£n l√Ω, ƒëi k√®m h·ªá th·ªëng logging, gi√°m s√°t ti·∫øn ƒë·ªô v√† ki·ªÉm tra ch·∫•t l∆∞·ª£ng chuy√™n nghi·ªáp.

## üìã **T·ªîNG QUAN H·ªÜ TH·ªêNG**

### **LawBot l√† g√¨?**
LawBot l√† m·ªôt h·ªá th·ªëng h·ªèi-ƒë√°p ph√°p lu·∫≠t ti√™n ti·∫øn ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·∫∑c bi·ªát cho ph√°p lu·∫≠t Vi·ªát Nam. H·ªá th·ªëng s·ª≠ d·ª•ng c√¥ng ngh·ªá AI hi·ªán ƒë·∫°i ƒë·ªÉ tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t m·ªôt c√°ch ch√≠nh x√°c v√† nhanh ch√≥ng.

### **Ki·∫øn tr√∫c h·ªá th·ªëng v7.0 (Cascaded Reranking):**
```
            [C√¢u h·ªèi ng∆∞·ªùi d√πng]
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ T·∫ßng 1: Bi-Encoder (Retrieval)            ‚îÇ
‚îÇ  - T√¨m ki·∫øm si√™u r·ªông, l·∫•y Top 500 ·ª©ng vi√™n ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ T·∫ßng 2: Light Reranker (Light Reranking) ‚îÇ
‚îÇ  - S√†ng l·ªçc si√™u nhanh, ch·ªçn Top 50        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ T·∫ßng 3: Ensemble Reranker (Strong Reranking)‚îÇ
‚îÇ  - H·ªôi ƒë·ªìng chuy√™n gia th·∫©m ƒë·ªãnh s√¢u       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
                    ‚ñº
            [Top 5 k·∫øt qu·∫£ ch√≠nh x√°c nh·∫•t]
```

### **C√°ch ho·∫°t ƒë·ªông:**
1. **üîç T·∫ßng 1 (Bi-Encoder)**: T√¨m 500 vƒÉn b·∫£n ph√°p lu·∫≠t li√™n quan nh·∫•t
2. **‚ö° T·∫ßng 2 (Light Reranker)**: L·ªçc nhanh xu·ªëng 50 ·ª©ng vi√™n ch·∫•t l∆∞·ª£ng cao
3. **‚öñÔ∏è T·∫ßng 3 (Ensemble)**: H·ªôi ƒë·ªìng chuy√™n gia th·∫©m ƒë·ªãnh v√† ch·ªçn top 5 k·∫øt qu·∫£ ch√≠nh x√°c nh·∫•t

## üéØ **T·∫†I SAO C·∫¶N LAW BOT? (WHY)**

### **V·∫•n ƒë·ªÅ hi·ªán t·∫°i:**
- ‚ùå **Kh√≥ t√¨m ki·∫øm**: VƒÉn b·∫£n ph√°p lu·∫≠t r·∫•t nhi·ªÅu v√† ph·ª©c t·∫°p
- ‚ùå **Th·ªùi gian ch·∫≠m**: T√¨m ki·∫øm th·ªß c√¥ng m·∫•t nhi·ªÅu th·ªùi gian
- ‚ùå **Thi·∫øu ch√≠nh x√°c**: K·∫øt qu·∫£ t√¨m ki·∫øm kh√¥ng ƒë√∫ng tr·ªçng t√¢m
- ‚ùå **Kh√≥ hi·ªÉu**: Ng√¥n ng·ªØ ph√°p l√Ω kh√≥ hi·ªÉu v·ªõi ng∆∞·ªùi kh√¥ng chuy√™n

### **Gi·∫£i ph√°p LawBot v7.0:**
- ‚úÖ **T√¨m ki·∫øm nhanh**: AI t√¨m ki·∫øm trong v√†i gi√¢y
- ‚úÖ **K·∫øt qu·∫£ ch√≠nh x√°c**: "Ph·ªÖu l·ªçc" 3 t·∫ßng ƒë·∫£m b·∫£o ƒë·ªô ch√≠nh x√°c t·ªëi ƒëa
- ‚úÖ **D·ªÖ hi·ªÉu**: Tr·∫£ v·ªÅ nh·ªØng ƒëi·ªÅu lu·∫≠t li√™n quan tr·ª±c ti·∫øp nh·∫•t
- ‚úÖ **Ti·∫øt ki·ªám th·ªùi gian**: Gi·∫£m th·ªùi gian tra c·ª©u t·ª´ v√†i gi·ªù xu·ªëng c√≤n v√†i gi√¢y

## ‚è∞ **KHI N√ÄO S·ª¨ D·ª§NG? (WHEN)**

### **C√°c tr∆∞·ªùng h·ª£p s·ª≠ d·ª•ng:**
- üîç **T√¨m ki·∫øm lu·∫≠t**: "Ng∆∞·ªùi lao ƒë·ªông ƒë∆∞·ª£c ngh·ªâ ph√©p bao nhi√™u ng√†y?"
- üìã **Tra c·ª©u quy ƒë·ªãnh**: "ƒêi·ªÅu ki·ªán th√†nh l·∫≠p doanh nghi·ªáp l√† g√¨?"
- ‚öñÔ∏è **So s√°nh vƒÉn b·∫£n**: "S·ª± kh√°c bi·ªát gi·ªØa Lu·∫≠t Doanh nghi·ªáp c≈© v√† m·ªõi?"
- üìù **T√¨m ƒëi·ªÅu kho·∫£n**: "ƒêi·ªÅu 113 B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh g√¨?"

### **Khi n√†o KH√îNG s·ª≠ d·ª•ng:**
- ‚ùå C√¢u h·ªèi kh√¥ng li√™n quan ƒë·∫øn ph√°p lu·∫≠t
- ‚ùå Y√™u c·∫ßu t∆∞ v·∫•n ph√°p l√Ω chuy√™n s√¢u
- ‚ùå Thay th·∫ø ho√†n to√†n lu·∫≠t s∆∞

## üìç **S·ª¨ D·ª§NG ·ªû ƒê√ÇU? (WHERE)**

### **M√¥i tr∆∞·ªùng ph√°t tri·ªÉn:**
- üíª **Local Development**: M√°y t√≠nh c√° nh√¢n
- üñ•Ô∏è **Server**: M√°y ch·ªß c√¥ng ty
- ‚òÅÔ∏è **Cloud**: AWS, Google Cloud, Azure
- üê≥ **Docker**: Container deployment

### **Y√™u c·∫ßu h·ªá th·ªëng:**
```
OS: Windows 10+, Ubuntu 18.04+, macOS 10.14+
RAM: T·ªëi thi·ªÉu 8GB, khuy·∫øn ngh·ªã 16GB+
Storage: T·ªëi thi·ªÉu 10GB cho models v√† data
GPU: NVIDIA GPU v·ªõi CUDA (khuy·∫øn ngh·ªã)
Python: 3.8+
```

## üë• **AI CHO? (WHO)**

### **ƒê·ªëi t∆∞·ª£ng s·ª≠ d·ª•ng:**
- üë®‚Äçüíº **Lu·∫≠t s∆∞**: Tra c·ª©u nhanh vƒÉn b·∫£n ph√°p lu·∫≠t
- üë®‚Äçüíª **Nh√† ph√°t tri·ªÉn**: T√≠ch h·ª£p v√†o ·ª©ng d·ª•ng ph√°p l√Ω
- üë®‚Äçüéì **Sinh vi√™n lu·∫≠t**: H·ªçc t·∫≠p v√† nghi√™n c·ª©u
- üë®‚Äçüíº **Doanh nghi·ªáp**: Tu√¢n th·ªß quy ƒë·ªãnh ph√°p lu·∫≠t
- üë®‚Äçüë©‚Äçüëß‚Äçüë¶ **C√¥ng d√¢n**: T√¨m hi·ªÉu quy·ªÅn v√† nghƒ©a v·ª•

### **ƒê·ªëi t∆∞·ª£ng ph√°t tri·ªÉn:**
- üë®‚Äçüíª **AI/ML Engineers**: Ph√°t tri·ªÉn v√† t·ªëi ∆∞u models
- üë®‚Äçüíª **Software Engineers**: T√≠ch h·ª£p v√† deployment
- üë®‚Äçüíª **Data Scientists**: Ph√¢n t√≠ch v√† c·∫£i thi·ªán performance
- üë®‚Äçüíª **DevOps Engineers**: Deployment v√† monitoring

## üöÄ **H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG (QUICK START)**

### **B∆∞·ªõc 1: C√†i ƒë·∫∑t M√¥i tr∆∞·ªùng**

```bash
# Clone repository
git clone https://github.com/lawbot-team/lawbot.git
cd lawbot

# T·∫°o m√¥i tr∆∞·ªùng ·∫£o
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate     # Windows

# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
pip install -r requirements.txt

# (T√πy ch·ªçn) C√†i ƒë·∫∑t th√™m dependencies cho development
pip install -r requirements-dev.txt
```

### **B∆∞·ªõc 2: Ch·∫°y To√†n b·ªô Pipeline (M·ªôt l·ªánh duy nh·∫•t)**

**üéØ L·ªánh khuy·∫øn ngh·ªã - Hi·ªáu su·∫•t cao nh·∫•t:**
```bash
python run_pipeline.py
```

**‚ö° L·ªánh nhanh - B·ªè qua DAPT:**
```bash
python run_pipeline.py --no-dapt
```

**üîß C√°c t√πy ch·ªçn kh√°c:**
```bash
# Xem danh s√°ch c√°c b∆∞·ªõc
python run_pipeline.py --list-steps

# Ch·∫°y t·ª´ b∆∞·ªõc c·ª• th·ªÉ
python run_pipeline.py --step 02

# B·ªè qua b∆∞·ªõc filtering
python run_pipeline.py --skip-filtering
```

### **B∆∞·ªõc 3: Ch·∫°y Giao di·ªán Web App**

Sau khi pipeline ho√†n t·∫•t, kh·ªüi ƒë·ªông giao di·ªán ng∆∞·ªùi d√πng:

```bash
streamlit run app.py
```

Truy c·∫≠p http://localhost:8501 ƒë·ªÉ b·∫Øt ƒë·∫ßu s·ª≠ d·ª•ng.

### **B∆∞·ªõc 4: S·ª≠ d·ª•ng API**

```python
from core.pipeline import LegalQAPipeline

# Kh·ªüi t·∫°o pipeline
pipeline = LegalQAPipeline()

# H·ªèi ƒë√°p
query = "Ng∆∞·ªùi lao ƒë·ªông ƒë∆∞·ª£c ngh·ªâ ph√©p bao nhi√™u ng√†y?"
results = pipeline.predict(query=query, top_k_retrieval=100, top_k_final=5)

# In k·∫øt qu·∫£
for i, result in enumerate(results):
    print(f"K·∫øt qu·∫£ {i+1}: {result['aid']}")
    print(f"ƒêi·ªÉm: {result['rerank_score']:.4f}")
    print(f"N·ªôi dung: {result['content'][:200]}...")
```





## üìÅ **C·∫§U TR√öC D·ª∞ √ÅN**

```
LawBot/
‚îú‚îÄ‚îÄ üìÅ app/
‚îÇ   ‚îî‚îÄ‚îÄ app.py                  # Giao di·ªán web Streamlit
‚îú‚îÄ‚îÄ üìÅ core/                    # C√°c module v√† class c·ªët l√µi
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py             # Class pipeline x·ª≠ l√Ω ch√≠nh (3 t·∫ßng)
‚îÇ   ‚îú‚îÄ‚îÄ logging_system.py       # H·ªá th·ªëng ghi log chuy√™n nghi·ªáp
‚îÇ   ‚îú‚îÄ‚îÄ evaluation_reporter.py  # C√¥ng c·ª• ƒë√°nh gi√° v√† t·∫°o b√°o c√°o
‚îÇ   ‚îú‚îÄ‚îÄ progress_tracker.py     # C√¥ng c·ª• theo d√µi ti·∫øn tr√¨nh
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/               # Package utilities th·ªëng nh·∫•t
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py         # Export t·∫•t c·∫£ utilities
‚îÇ       ‚îú‚îÄ‚îÄ data_processing.py  # X·ª≠ l√Ω d·ªØ li·ªáu (load, save, parse)
‚îÇ       ‚îú‚îÄ‚îÄ model_utils.py      # Utilities cho models (preprocessing, metrics)
‚îÇ       ‚îú‚îÄ‚îÄ evaluation.py       # Metrics ƒë√°nh gi√° (precision, recall, MRR)
‚îÇ       ‚îú‚îÄ‚îÄ augmentation.py     # Data augmentation utilities
‚îÇ       ‚îî‚îÄ‚îÄ file_utils.py       # File v√† path management
‚îú‚îÄ‚îÄ üìÅ scripts/                 # C√°c k·ªãch b·∫£n th·ª±c thi pipeline (v8.1 optimized)
‚îÇ   ‚îú‚îÄ‚îÄ 00_adapt_model.py       # (N√¢ng cao) DAPT v·ªõi GPU acceleration
‚îÇ   ‚îú‚îÄ‚îÄ 01_check_environment.py # B∆∞·ªõc 1: M√¥i tr∆∞·ªùng & S∆° ch·∫ø
‚îÇ   ‚îú‚îÄ‚îÄ 02_prepare_training_data.py # B∆∞·ªõc 2: Hard negative mining t·ªëi ∆∞u
‚îÇ   ‚îú‚îÄ‚îÄ 03_train_models.py      # B∆∞·ªõc 3: Training v·ªõi single data loading
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/               # C√°c script ti·ªán √≠ch
‚îÇ       ‚îú‚îÄ‚îÄ filter_dataset.py   # Logic l·ªçc d·ªØ li·ªáu
‚îÇ       ‚îú‚îÄ‚îÄ run_filter.py       # Wrapper ƒë·ªÉ ch·∫°y filter
‚îÇ       ‚îî‚îÄ‚îÄ check_project.py    # Ki·ªÉm tra c·∫•u tr√∫c d·ª± √°n
‚îú‚îÄ‚îÄ üìÅ data/
‚îú‚îÄ‚îÄ üìÅ models/                  # N∆°i l∆∞u c√°c model ƒë√£ hu·∫•n luy·ªán
‚îÇ   ‚îú‚îÄ‚îÄ phobert-law/            # (N√¢ng cao) Model chuy√™n gia ph√°p lu·∫≠t
‚îÇ   ‚îú‚îÄ‚îÄ light-reranker/         # Model reranker si√™u nhanh
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ üìÅ indexes/                 # N∆°i l∆∞u FAISS index
‚îú‚îÄ‚îÄ üìÅ reports/                 # C√°c b√°o c√°o ƒë√°nh gi√°
‚îú‚îÄ‚îÄ üìÅ logs/                    # C√°c file log c·ªßa m·ªói l·∫ßn ch·∫°y
‚îú‚îÄ‚îÄ üìÑ config.py                # File c·∫•u h√¨nh trung t√¢m (t·ªëi ∆∞u h√≥a)
‚îú‚îÄ‚îÄ üìÑ run_pipeline.py          # Tr√¨nh ƒëi·ªÅu khi·ªÉn pipeline ch√≠nh
‚îî‚îÄ‚îÄ üìÑ README.md                # T√†i li·ªáu h∆∞·ªõng d·∫´n
```

## üõ†Ô∏è **C·∫§U H√åNH & T√ôY CH·ªàNH**

T·∫•t c·∫£ c√°c tham s·ªë quan tr·ªçng c·ªßa h·ªá th·ªëng ƒë·ªÅu ƒë∆∞·ª£c qu·∫£n l√Ω t·∫≠p trung t·∫°i `config.py`. B·∫°n c√≥ th·ªÉ d·ªÖ d√†ng thay ƒë·ªïi model, ƒëi·ªÅu ch·ªânh si√™u tham s·ªë hu·∫•n luy·ªán (learning rate, batch size) v√† c√°c c√†i ƒë·∫∑t c·ªßa pipeline t·∫°i ƒë√¢y.

H·ªá th·ªëng c≈©ng h·ªó tr·ª£ c·∫•u h√¨nh qua **Bi·∫øn M√¥i tr∆∞·ªùng (Environment Variables)**, r·∫•t h·ªØu √≠ch khi tri·ªÉn khai l√™n server.

### **Environment Variables**

```bash
# Environment
export LAWBOT_ENV=production
export LAWBOT_DEBUG=false

# Directories
export LAWBOT_DATA_DIR=/path/to/data
export LAWBOT_MODELS_DIR=/path/to/models
export LAWBOT_INDEXES_DIR=/path/to/indexes

# Hyperparameters (T·ªëi ∆∞u h√≥a v8.0)
export LAWBOT_BI_ENCODER_BATCH_SIZE=16      # TƒÉng t·ª´ 4 l√™n 16
export LAWBOT_CROSS_ENCODER_BATCH_SIZE=8    # TƒÉng t·ª´ 4 l√™n 8
export LAWBOT_BI_ENCODER_LR=2e-5            # TƒÉng t·ª´ 1e-7 l√™n 2e-5
export LAWBOT_CROSS_ENCODER_LR=2e-5         # TƒÉng t·ª´ 5e-6 l√™n 2e-5
export LAWBOT_BI_ENCODER_EPOCHS=3           # TƒÉng t·ª´ 1 l√™n 3
export LAWBOT_CROSS_ENCODER_EPOCHS=5        # TƒÉng t·ª´ 1 l√™n 5

# Performance Optimizations (M·ªõi v8.0)
export LAWBOT_FP16_TRAINING=true            # Mixed Precision Training
export LAWBOT_BI_ENCODER_DATALOADER_NUM_WORKERS=4
export LAWBOT_CROSS_ENCODER_DATALOADER_NUM_WORKERS=4
export LAWBOT_BI_ENCODER_DATALOADER_PIN_MEMORY=true
export LAWBOT_CROSS_ENCODER_DATALOADER_PIN_MEMORY=true

# Pipeline settings
export LAWBOT_TOP_K_RETRIEVAL=100
export LAWBOT_TOP_K_FINAL=5
```

### **Configuration File**

```python
import config

# In th√¥ng tin c·∫•u h√¨nh
config.print_config_summary()

# Validate c·∫•u h√¨nh
config.validate_config()
```

## üîÑ **LU·ªíNG X·ª¨ L√ù NGHI·ªÜP V·ª§ CHI TI·∫æT**

### **üéØ Quy tr√¨nh x·ª≠ l√Ω c√¢u h·ªèi ph√°p lu·∫≠t (Inference Pipeline):**

H√£y t∆∞·ªüng t∆∞·ª£ng LawBot nh∆∞ m·ªôt th∆∞ vi·ªán ph√°p lu·∫≠t kh·ªïng l·ªì c√≥ hai b·ªô ph·∫≠n ch√≠nh:

#### **1. B·ªô ph·∫≠n "Tra c·ª©u" (Inference/Request)**
ƒê√¢y l√† khi c√≥ ng∆∞·ªùi ƒë·∫øn h·ªèi v√† c√°c "th·ªß th∆∞ AI" nhanh ch√≥ng t√¨m ra c√¢u tr·∫£ l·ªùi ch√≠nh x√°c. Qu√° tr√¨nh n√†y di·ªÖn ra m·ªói khi ng∆∞·ªùi d√πng ƒë·∫∑t c√¢u h·ªèi.

**B·ªëi c·∫£nh**: B·∫°n m·ªü tr√¨nh duy·ªát, truy c·∫≠p v√†o giao di·ªán web do `app.py` t·∫°o ra v√† g√µ c√¢u h·ªèi: *"Ng∆∞·ªùi lao ƒë·ªông ƒë∆∞·ª£c ngh·ªâ ph√©p bao nhi√™u ng√†y?"*

**H√†nh tr√¨nh chi ti·∫øt**:

* **B∆∞·ªõc 1: Giao di·ªán nh·∫≠n c√¢u h·ªèi (`app.py`)**
  * Giao di·ªán web ƒë∆∞·ª£c t·∫°o b·ªüi Streamlit trong file `app.py` s·∫Ω nh·∫≠n c√¢u h·ªèi c·ªßa b·∫°n.
  * Khi b·∫°n nh·∫•n n√∫t "T√¨m ki·∫øm", n√≥ s·∫Ω g·ªçi h√†m `pipeline.predict()` v√† truy·ªÅn c√¢u h·ªèi c·ªßa b·∫°n v√†o.
  * `pipeline` ·ªü ƒë√¢y l√† m·ªôt th·ª±c th·ªÉ c·ªßa class `LegalQAPipeline` ƒë√£ ƒë∆∞·ª£c t·∫£i s·∫µn.

* **B∆∞·ªõc 2: Pipeline b·∫Øt ƒë·∫ßu x·ª≠ l√Ω (`core/pipeline.py`)**
  * H√†m `predict()` trong `LegalQAPipeline` nh·∫≠n nhi·ªám v·ª•.
  * N√≥ s·∫Ω ƒëi·ªÅu ph·ªëi m·ªôt quy tr√¨nh "ph·ªÖu l·ªçc" 3 t·∫ßng tinh vi c√≥ t√™n l√† **Cascaded Reranking** ƒë·ªÉ ƒë·∫£m b·∫£o k·∫øt qu·∫£ v·ª´a nhanh v·ª´a ch√≠nh x√°c.

* **B∆∞·ªõc 3: T·∫ßng 1 - T√¨m ki·∫øm R·ªông (Retrieval)**
  * **M·ª•c ti√™u**: Nhanh ch√≥ng t√¨m ra kho·∫£ng 500 vƒÉn b·∫£n lu·∫≠t c√≥ v·∫ª li√™n quan nh·∫•t t·ª´ to√†n b·ªô kho d·ªØ li·ªáu.
  * **C√°ch l√†m**:
    1. H√†m `retrieve()` ƒë∆∞·ª£c g·ªçi.
    2. M√¥ h√¨nh `Bi-Encoder` (m·ªôt chuy√™n gia t·ªëc ƒë·ªô) s·∫Ω "ƒë·ªçc" c√¢u h·ªèi c·ªßa b·∫°n v√† chuy·ªÉn n√≥ th√†nh m·ªôt vector (m·ªôt chu·ªói s·ªë ƒë·∫°i di·ªán cho √Ω nghƒ©a).
    3. Vector n√†y ƒë∆∞·ª£c ƒëem ƒëi so s√°nh v·ªõi h√†ng ng√†n vector c·ªßa c√°c vƒÉn b·∫£n lu·∫≠t ƒë√£ ƒë∆∞·ª£c m√£ h√≥a v√† l∆∞u s·∫µn trong m·ªôt "cu·ªën s·ªï tra c·ª©u si√™u t·ªëc" g·ªçi l√† `FAISS index`.
    4. FAISS index tr·∫£ v·ªÅ 500 vƒÉn b·∫£n c√≥ vector g·∫ßn gi·ªëng v·ªõi vector c√¢u h·ªèi nh·∫•t, c√πng v·ªõi ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng ban ƒë·∫ßu.

* **B∆∞·ªõc 4: T·∫ßng 2 - S√†ng l·ªçc Nhanh (Light Reranking)**
  * **M·ª•c ti√™u**: Gi·∫£m t·∫£i cho t·∫ßng cu·ªëi b·∫±ng c√°ch l·ªçc t·ª´ 500 ·ª©ng vi√™n xu·ªëng c√≤n kho·∫£ng 50 ·ª©ng vi√™n ch·∫•t l∆∞·ª£ng cao.
  * **C√°ch l√†m**:
    1. H√†m `rerank_light()` ƒë∆∞·ª£c g·ªçi (n·∫øu `use_cascaded_reranking` ƒë∆∞·ª£c b·∫≠t).
    2. M·ªôt m√¥ h√¨nh nh·ªè v√† nhanh g·ªçi l√† `Light Reranker` s·∫Ω ƒë·ªçc l∆∞·ªõt qua c·∫∑p (c√¢u h·ªèi, t·ª´ng vƒÉn b·∫£n trong 500 ·ª©ng vi√™n) ƒë·ªÉ cho m·ªôt ƒëi·ªÉm ƒë√°nh gi√° s∆° b·ªô.
    3. H·ªá th·ªëng k·∫øt h·ª£p ƒëi·ªÉm s∆° b·ªô n√†y v·ªõi ƒëi·ªÉm t·ª´ T·∫ßng 1 ƒë·ªÉ ra m·ªôt ƒëi·ªÉm t·ªïng h·ª£p, sau ƒë√≥ ch·ªçn ra 50 ·ª©ng vi√™n c√≥ ƒëi·ªÉm cao nh·∫•t.

* **B∆∞·ªõc 5: T·∫ßng 3 - Th·∫©m ƒë·ªãnh Chuy√™n s√¢u (Strong Reranking)**
  * **M·ª•c ti√™u**: ƒê·∫°t ƒë·ªô ch√≠nh x√°c t·ªëi ƒëa b·∫±ng c√°ch ƒë·ªÉ m·ªôt "h·ªôi ƒë·ªìng chuy√™n gia" th·∫©m ƒë·ªãnh k·ªπ l∆∞·ª°ng 50 ·ª©ng vi√™n c√≤n l·∫°i.
  * **C√°ch l√†m**:
    1. H√†m `rerank()` ƒë∆∞·ª£c g·ªçi.
    2. **H·ªôi ƒë·ªìng chuy√™n gia (Ensemble)**: Thay v√¨ ch·ªâ d√πng m·ªôt m√¥ h√¨nh, h·ªá th·ªëng s·ª≠ d·ª•ng nhi·ªÅu m√¥ h√¨nh `Cross-Encoder` m·∫°nh (v√≠ d·ª•: `PhoBERT-Law` ƒë√£ ƒë∆∞·ª£c "chuy√™n m√¥n h√≥a" v√† m·ªôt m√¥ h√¨nh kh√°c nh∆∞ `XLM-RoBERTa`) c√πng l√†m vi·ªác.
    3. T·ª´ng chuy√™n gia trong h·ªôi ƒë·ªìng s·∫Ω ƒë·ªçc r·∫•t k·ªπ t·ª´ng c·∫∑p (c√¢u h·ªèi, vƒÉn b·∫£n) v√† cho m·ªôt ƒëi·ªÉm s·ªë chi ti·∫øt v·ªÅ m·ª©c ƒë·ªô li√™n quan.
    4. **X·ª≠ l√Ω vƒÉn b·∫£n d√†i**: N·∫øu m·ªôt vƒÉn b·∫£n qu√° d√†i, h·ªá th·ªëng s·∫Ω t·ª± ƒë·ªông c·∫Øt n√≥ th√†nh c√°c "chunk" (ƒëo·∫°n) nh·ªè h∆°n ƒë·ªÉ c√°c m√¥ h√¨nh c√≥ th·ªÉ ƒë·ªçc h·∫øt m√† kh√¥ng b·ªè s√≥t th√¥ng tin.
    5. ƒêi·ªÉm s·ªë cu·ªëi c√πng c·ªßa m·ªói vƒÉn b·∫£n s·∫Ω l√† ƒëi·ªÉm trung b√¨nh t·ª´ t·∫•t c·∫£ c√°c chuy√™n gia (ho·∫∑c ƒëi·ªÉm cao nh·∫•t t·ª´ c√°c chunk c·ªßa n√≥).

* **B∆∞·ªõc 6: Tr·∫£ k·∫øt qu·∫£ v·ªÅ giao di·ªán (`app.py`)**
  * H√†m `predict()` tr·∫£ v·ªÅ danh s√°ch c√°c vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c h·ªôi ƒë·ªìng chuy√™n gia x·∫øp h·∫°ng cao nh·∫•t.
  * Giao di·ªán `app.py` nh·∫≠n l·∫•y danh s√°ch n√†y v√† hi·ªÉn th·ªã m·ªôt c√°ch ƒë·∫πp ƒë·∫Ω, d·ªÖ ƒë·ªçc cho b·∫°n, bao g·ªìm n·ªôi dung ƒëi·ªÅu lu·∫≠t v√† ƒëi·ªÉm s·ªë ƒë·ªÉ b·∫°n bi·∫øt m·ª©c ƒë·ªô tin c·∫≠y.

### **üß† Lu·ªìng Logic Hu·∫•n luy·ªán M√¥ h√¨nh (Training Pipeline):**

ƒê√¢y l√† qu√° tr√¨nh "d·∫°y h·ªçc" cho c√°c AI, di·ªÖn ra m·ªôt l·∫ßn tr∆∞·ªõc khi h·ªá th·ªëng c√≥ th·ªÉ s·ª≠ d·ª•ng. To√†n b·ªô qu√° tr√¨nh n√†y ƒë∆∞·ª£c ƒëi·ªÅu khi·ªÉn b·ªüi file `run_pipeline.py`.

**B·ªëi c·∫£nh**: B·∫°n l√† nh√† ph√°t tri·ªÉn, v·ª´a t·∫£i m√£ ngu·ªìn v·ªÅ v√† c√≥ trong tay b·ªô d·ªØ li·ªáu th√¥ (`legal_corpus.json`, `train.json`). B·∫°n m·ªü terminal v√† chu·∫©n b·ªã ch·∫°y.

**H√†nh tr√¨nh chi ti·∫øt**:

* **B∆∞·ªõc 0 (T√πy ch·ªçn nh∆∞ng quan tr·ªçng): Chuy√™n m√¥n h√≥a Ng√¥n ng·ªØ (`00_adapt_model.py`)**
  * **M·ª•c ti√™u**: Bi·∫øn m√¥ h√¨nh `PhoBERT` (m·ªôt chuy√™n gia ng√¥n ng·ªØ t·ªïng qu√°t) th√†nh `PhoBERT-Law` (m·ªôt chuy√™n gia am hi·ªÉu s√¢u v·ªÅ thu·∫≠t ng·ªØ ph√°p l√Ω). Qu√° tr√¨nh n√†y g·ªçi l√† **Domain-Adaptive Pre-training (DAPT)**.
  * **C√°ch l√†m**: Script n√†y cho m√¥ h√¨nh `PhoBERT` ƒë·ªçc to√†n b·ªô kho vƒÉn b·∫£n lu·∫≠t trong `legal_corpus.json` v√† h·ªçc l·∫°i c√°ch c√°c t·ª´ ng·ªØ ƒë∆∞·ª£c s·ª≠ d·ª•ng trong b·ªëi c·∫£nh ph√°p lu·∫≠t. K·∫øt qu·∫£ l√† m·ªôt m√¥ h√¨nh m·ªõi, "th√¥ng th·∫°o" lu·∫≠t h∆°n, ƒë∆∞·ª£c l∆∞u l·∫°i ƒë·ªÉ c√°c b∆∞·ªõc sau s·ª≠ d·ª•ng.

* **B∆∞·ªõc 1: Chu·∫©n b·ªã M√¥i tr∆∞·ªùng v√† D·ªØ li·ªáu (`01_check_environment.py`)**
  * **M·ª•c ti√™u**: D·ªçn d·∫πp "s√¢n ch∆°i" v√† chu·∫©n b·ªã nguy√™n li·ªáu.
  * **C√°ch l√†m**: Script n√†y ki·ªÉm tra xem b·∫°n ƒë√£ c√†i ƒë·ªß c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ch∆∞a, c√°c file d·ªØ li·ªáu c√≥ t·ªìn t·∫°i kh√¥ng. Quan tr·ªçng nh·∫•t, n√≥ ƒë·ªçc d·ªØ li·ªáu th√¥ v√† t·∫°o ra c√°c "b·∫£n ƒë·ªì" (`aid_map`, `doc_id_to_aids`) ƒë·ªÉ d·ªÖ d√†ng tra c·ª©u n·ªôi dung vƒÉn b·∫£n t·ª´ ID c·ªßa n√≥, ƒë·ªìng th·ªùi chia d·ªØ li·ªáu ra th√†nh 2 t·∫≠p: m·ªôt ƒë·ªÉ hu·∫•n luy·ªán (train), m·ªôt ƒë·ªÉ ki·ªÉm tra (validation).

* **B∆∞·ªõc 2: Chu·∫©n b·ªã D·ªØ li·ªáu Hu·∫•n luy·ªán N√¢ng cao (`02_prepare_training_data.py`)**
  * **M·ª•c ti√™u**: T·∫°o ra "b√†i t·∫≠p" ch·∫•t l∆∞·ª£ng cao ƒë·ªÉ d·∫°y cho c√°c m√¥ h√¨nh AI.
  * **C√°ch l√†m**: ƒê√¢y l√† m·ªôt b∆∞·ªõc c·ª±c k·ª≥ th√¥ng minh.
    1. **T·∫°o "b√†i t·∫≠p d·ªÖ"**: ƒê·∫ßu ti√™n, n√≥ t·∫°o ra c√°c b·ªô ba (c√¢u h·ªèi, c√¢u tr·∫£ l·ªùi ƒë√∫ng, c√¢u tr·∫£ l·ªùi sai ng·∫´u nhi√™n). ƒê√¢y l√† nh·ªØng b√†i t·∫≠p c∆° b·∫£n.
    2. **ƒê√†o t·∫°o "gi√°o vi√™n t·∫°m th·ªùi"**: N√≥ d√πng nh·ªØng b√†i t·∫≠p d·ªÖ n√†y ƒë·ªÉ hu·∫•n luy·ªán nhanh m·ªôt m√¥ h√¨nh `Bi-Encoder` t·∫°m th·ªùi.
    3. **T√¨m "b√†i t·∫≠p kh√≥" (Hard Negative Mining)**: N√≥ d√πng "gi√°o vi√™n t·∫°m th·ªùi" n√†y ƒë·ªÉ t√¨m ki·∫øm c√°c c√¢u tr·∫£ l·ªùi *sai* nh∆∞ng l·∫°i c√≥ v·∫ª *r·∫•t gi·ªëng* v·ªõi c√¢u tr·∫£ l·ªùi ƒë√∫ng. ƒê√¢y ch√≠nh l√† c√°c "b·∫´y" ng·ªØ nghƒ©a m√† AI c·∫ßn ph·∫£i h·ªçc ƒë·ªÉ v∆∞·ª£t qua.
    4. **T·∫°o b·ªô b√†i t·∫≠p cu·ªëi c√πng**: N√≥ k·∫øt h·ª£p c·∫£ "b√†i t·∫≠p d·ªÖ" v√† "b√†i t·∫≠p kh√≥" ƒë·ªÉ t·∫°o ra b·ªô d·ªØ li·ªáu hu·∫•n luy·ªán cu·ªëi c√πng, gi√∫p AI tr·ªü n√™n th√¥ng minh v√† tinh vi h∆°n. D·ªØ li·ªáu n√†y ƒë∆∞·ª£c l∆∞u l·∫°i cho c·∫£ `Bi-Encoder` (d·∫°ng triplets) v√† `Cross-Encoder` (d·∫°ng pairs).

* **B∆∞·ªõc 3: Hu·∫•n luy·ªán v√† ƒê√°nh gi√° (`03_train_models.py`)**
  * **M·ª•c ti√™u**: D√πng b·ªô "b√†i t·∫≠p" ch·∫•t l∆∞·ª£ng cao ƒë·ªÉ hu·∫•n luy·ªán t·∫•t c·∫£ c√°c m√¥ h√¨nh ch√≠nh th·ª©c.
  * **C√°ch l√†m**:
    1. **Hu·∫•n luy·ªán Bi-Encoder**: D·∫°y cho "chuy√™n gia t·ªëc ƒë·ªô" c√°ch ph√¢n bi·ªát c√¢u tr·∫£ l·ªùi ƒë√∫ng v√† sai t·ª´ c√°c b√†i t·∫≠p triplets ƒë√£ t·∫°o.
    2. **X√¢y d·ª±ng FAISS Index**: D√πng `Bi-Encoder` v·ª´a ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ t·∫°o vector cho t·∫•t c·∫£ vƒÉn b·∫£n lu·∫≠t v√† x√¢y d·ª±ng n√™n "cu·ªën s·ªï tra c·ª©u si√™u t·ªëc" FAISS.
    3. **Hu·∫•n luy·ªán Light Reranker**: D·∫°y cho "ng∆∞·ªùi s√†ng l·ªçc nhanh" c√°ch ƒë√°nh gi√° s∆° b·ªô.
    4. **Hu·∫•n luy·ªán Cross-Encoder (H·ªôi ƒë·ªìng chuy√™n gia)**: D·∫°y cho c√°c "chuy√™n gia th·∫©m ƒë·ªãnh" c√°ch ph√¢n t√≠ch s√¢u. N·∫øu `PhoBERT-Law` t·ª´ B∆∞·ªõc 0 t·ªìn t·∫°i, n√≥ s·∫Ω ƒë∆∞·ª£c d√πng l√†m n·ªÅn t·∫£ng ƒë·ªÉ hu·∫•n luy·ªán, t·∫°o ra m·ªôt chuy√™n gia c·ª±c k·ª≥ s·∫Øc b√©n.
    5. **ƒê√°nh gi√°**: Sau khi hu·∫•n luy·ªán, script s·∫Ω t·ª± ƒë·ªông ch·∫°y c√°c b√†i ki·ªÉm tra (s·ª≠ d·ª•ng `evaluation_utils.py`) ƒë·ªÉ b√°o c√°o xem c√°c m√¥ h√¨nh ho·∫°t ƒë·ªông t·ªët ƒë·∫øn ƒë√¢u.

## üöÄ **H∆Ø·ªöNG D·∫™N B·∫ÆT ƒê·∫¶U NHANH (QUICK START GUIDE)**

### **C√°ch Ch·∫°y Project L·∫ßn ƒë·∫ßu Hi·ªáu qu·∫£**

ƒê·ªÉ kh·ªüi ƒë·ªông d·ª± √°n n√†y, b·∫°n ch·ªâ c·∫ßn l√†m theo c√°c b∆∞·ªõc ƒë∆°n gi·∫£n sau:

#### **B∆∞·ªõc 1: Chu·∫©n b·ªã M√¥i tr∆∞·ªùng**
* T·∫£i m√£ ngu·ªìn v·ªÅ m√°y: `git clone https://github.com/lawbot-team/lawbot.git`
* ƒêi v√†o th∆∞ m·ª•c d·ª± √°n: `cd lawbot`
* T·∫°o m·ªôt m√¥i tr∆∞·ªùng ·∫£o ƒë·ªÉ kh√¥ng l√†m ·∫£nh h∆∞·ªüng ƒë·∫øn c√°c th∆∞ vi·ªán Python kh√°c tr√™n m√°y c·ªßa b·∫°n: `python -m venv venv`
* K√≠ch ho·∫°t m√¥i tr∆∞·ªùng ·∫£o:
  * Tr√™n Windows: `venv\Scripts\activate`
  * Tr√™n Linux/Mac: `source venv/bin/activate`
* C√†i ƒë·∫∑t t·∫•t c·∫£ c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ch·ªâ b·∫±ng m·ªôt l·ªánh: `pip install -r requirements.txt`

#### **B∆∞·ªõc 2: Chu·∫©n b·ªã D·ªØ li·ªáu**
* H√£y ch·∫Øc ch·∫Øn r·∫±ng b·∫°n c√≥ c√°c file d·ªØ li·ªáu th√¥ (`legal_corpus.json`, `train.json`, `public_test.json`).
* ƒê·∫∑t ch√∫ng v√†o ƒë√∫ng v·ªã tr√≠: th∆∞ m·ª•c `data/raw/`. C·∫•u tr√∫c th∆∞ m·ª•c n√†y ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong file `config.py`.

#### **B∆∞·ªõc 3: Ch·∫°y To√†n b·ªô Pipeline Hu·∫•n luy·ªán**
* B√¢y gi·ªù l√† ph·∫ßn th√∫ v·ªã nh·∫•t. B·∫°n ch·ªâ c·∫ßn ch·∫°y m·ªôt l·ªánh duy nh·∫•t trong terminal:
```bash
python run_pipeline.py
```
* L·ªánh n√†y s·∫Ω t·ª± ƒë·ªông th·ª±c hi·ªán t·∫•t c·∫£ c√°c b∆∞·ªõc hu·∫•n luy·ªán t·ª´ 0 ƒë·∫øn 3 m√† t√¥i ƒë√£ gi·∫£i th√≠ch ·ªü tr√™n. N√≥ s·∫Ω t·ª± ƒë·ªông chuy√™n m√¥n h√≥a m√¥ h√¨nh, chu·∫©n b·ªã d·ªØ li·ªáu, t√¨m hard negatives, v√† hu·∫•n luy·ªán t·∫•t c·∫£ c√°c AI. Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t kh√° nhi·ªÅu th·ªùi gian (v√†i gi·ªù) t√πy thu·ªôc v√†o s·ª©c m·∫°nh m√°y t√≠nh c·ªßa b·∫°n, ƒë·∫∑c bi·ªát l√† ·ªü b∆∞·ªõc 0 (DAPT) v√† b∆∞·ªõc 3 (Training).

* **M·∫πo ƒë·ªÉ ch·∫°y nhanh h∆°n l·∫ßn ƒë·∫ßu**: N·∫øu b·∫°n mu·ªën th·∫•y k·∫øt qu·∫£ nhanh h∆°n, b·∫°n c√≥ th·ªÉ b·ªè qua b∆∞·ªõc DAPT (B∆∞·ªõc 0) t·ªën nhi·ªÅu th·ªùi gian b·∫±ng l·ªánh:
```bash
python run_pipeline.py --no-dapt
```
H·ªá th·ªëng v·∫´n s·∫Ω ho·∫°t ƒë·ªông t·ªët, ch·ªâ l√† ƒë·ªô ch√≠nh x√°c c√≥ th·ªÉ kh√¥ng ·ªü m·ª©c t·ªëi ƒëa.

#### **B∆∞·ªõc 4: Kh·ªüi ƒë·ªông Giao di·ªán Web**
* Sau khi pipeline ·ªü B∆∞·ªõc 3 ch·∫°y xong v√† b√°o th√†nh c√¥ng, t·∫•t c·∫£ c√°c m√¥ h√¨nh AI c·ªßa b·∫°n ƒë√£ s·∫µn s√†ng.
* Ch·∫°y l·ªánh sau ƒë·ªÉ kh·ªüi ƒë·ªông giao di·ªán web:
```bash
streamlit run app.py
```
* M·ªü tr√¨nh duy·ªát v√† truy c·∫≠p v√†o ƒë·ªãa ch·ªâ `http://localhost:8501`. B√¢y gi·ªù b·∫°n c√≥ th·ªÉ tr·ª±c ti·∫øp ƒë·∫∑t c√¢u h·ªèi v√† chi√™m ng∆∞·ª°ng th√†nh qu·∫£ c·ªßa m√¨nh!

### **C√°c T√πy ch·ªçn Ch·∫°y Kh√°c**

#### **Ch·∫°y t·ª´ng b∆∞·ªõc ri√™ng l·∫ª:**
```bash
# B∆∞·ªõc 0: DAPT (Domain-Adaptive Pre-training)
python scripts/00_adapt_model.py

# B∆∞·ªõc 1: Ki·ªÉm tra m√¥i tr∆∞·ªùng v√† x·ª≠ l√Ω d·ªØ li·ªáu
python scripts/01_check_environment.py

# B∆∞·ªõc 2: Chu·∫©n b·ªã d·ªØ li·ªáu training
python scripts/02_prepare_training_data.py

# B∆∞·ªõc 3: Hu·∫•n luy·ªán models v√† ƒë√°nh gi√°
python scripts/03_train_models.py
```

#### **C√°c t√πy ch·ªçn kh√°c:**
```bash
# Xem danh s√°ch c√°c b∆∞·ªõc
python run_pipeline.py --list-steps

# Ch·∫°y t·ª´ b∆∞·ªõc c·ª• th·ªÉ
python run_pipeline.py --step 02

# B·ªè qua b∆∞·ªõc filtering
python run_pipeline.py --skip-filtering
```

## üîÑ **LU·ªíNG X·ª¨ L√ù NGHI·ªÜP V·ª§ CHI TI·∫æT**

### **üéØ Quy tr√¨nh x·ª≠ l√Ω c√¢u h·ªèi ph√°p lu·∫≠t:**

#### **1. Ti·∫øp nh·∫≠n c√¢u h·ªèi**
- **Input**: Ng∆∞·ªùi d√πng nh·∫≠p c√¢u h·ªèi v·ªÅ ph√°p lu·∫≠t
- **V√≠ d·ª•**: "Ng∆∞·ªùi lao ƒë·ªông ƒë∆∞·ª£c ngh·ªâ ph√©p bao nhi√™u ng√†y?"
- **X·ª≠ l√Ω**: H·ªá th·ªëng ph√¢n t√≠ch v√† chu·∫©n h√≥a c√¢u h·ªèi

#### **2. T·∫ßng 1: T√¨m ki·∫øm r·ªông (Bi-Encoder)**
- **M·ª•c ƒë√≠ch**: T√¨m 500 vƒÉn b·∫£n ph√°p lu·∫≠t c√≥ li√™n quan nh·∫•t
- **C√°ch ho·∫°t ƒë·ªông**: 
  - Chuy·ªÉn c√¢u h·ªèi th√†nh vector 768 chi·ªÅu
  - So s√°nh v·ªõi t·∫•t c·∫£ vƒÉn b·∫£n trong kho d·ªØ li·ªáu
  - Tr·∫£ v·ªÅ 500 k·∫øt qu·∫£ c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t
- **Th·ªùi gian**: ~100ms
- **ƒê·ªô ch√≠nh x√°c**: ~70% (Precision@5)

#### **3. T·∫ßng 2: L·ªçc nhanh (Light Reranker)**
- **M·ª•c ƒë√≠ch**: L·ªçc nhanh t·ª´ 500 xu·ªëng 50 ·ª©ng vi√™n ch·∫•t l∆∞·ª£ng cao
- **C√°ch ho·∫°t ƒë·ªông**:
  - S·ª≠ d·ª•ng model nh·ªè, nhanh ƒë·ªÉ ƒë√°nh gi√° s∆° b·ªô
  - K·∫øt h·ª£p ƒëi·ªÉm retrieval v·ªõi ƒëi·ªÉm light reranking
  - Ch·ªçn top 50 ·ª©ng vi√™n ƒë·ªÉ ƒë∆∞a l√™n t·∫ßng 3
- **Th·ªùi gian**: ~150ms
- **L√Ω do**: Ti·∫øt ki·ªám th·ªùi gian cho t·∫ßng 3

#### **4. T·∫ßng 3: Th·∫©m ƒë·ªãnh chuy√™n s√¢u (Ensemble)**
- **M·ª•c ƒë√≠ch**: H·ªôi ƒë·ªìng chuy√™n gia th·∫©m ƒë·ªãnh v√† ch·ªçn top 5 k·∫øt qu·∫£
- **C√°ch ho·∫°t ƒë·ªông**:
  - S·ª≠ d·ª•ng nhi·ªÅu model Cross-Encoder c√πng l√∫c
  - PhoBERT-Law + XLM-RoBERTa ƒë√°nh gi√° song song
  - L·∫•y ƒëi·ªÉm trung b√¨nh ƒë·ªÉ ra quy·∫øt ƒë·ªãnh cu·ªëi c√πng
- **Th·ªùi gian**: ~300ms
- **ƒê·ªô ch√≠nh x√°c**: >90% (Precision@5)

#### **5. Tr·∫£ v·ªÅ k·∫øt qu·∫£**
- **Output**: 5 vƒÉn b·∫£n ph√°p lu·∫≠t ph√π h·ª£p nh·∫•t
- **Th√¥ng tin bao g·ªìm**:
  - N·ªôi dung ƒëi·ªÅu lu·∫≠t
  - ƒêi·ªÉm s·ªë t·ª´ng t·∫ßng
  - Th√¥ng tin b·ªï sung (n·∫øu c√≥)

### **üß† Logic nghi·ªáp v·ª• chi ti·∫øt:**

#### **T·∫°i sao c·∫ßn 3 t·∫ßng?**
1. **T·∫ßng 1**: Kh√¥ng th·ªÉ b·ªè qua v√¨ c·∫ßn t√¨m ki·∫øm trong to√†n b·ªô kho d·ªØ li·ªáu
2. **T·∫ßng 2**: C·∫ßn thi·∫øt ƒë·ªÉ gi·∫£m t·∫£i cho t·∫ßng 3, tr√°nh l√£ng ph√≠ t√†i nguy√™n
3. **T·∫ßng 3**: C·∫ßn thi·∫øt ƒë·ªÉ ƒë·∫°t ƒë·ªô ch√≠nh x√°c t·ªëi ƒëa cho k·∫øt qu·∫£ cu·ªëi c√πng

#### **C√°ch h·ªá th·ªëng h·ªçc h·ªèi:**
1. **Hard Negative Mining**: T·ª± ƒë·ªông t√¨m nh·ªØng v√≠ d·ª• kh√≥ nh·∫•t ƒë·ªÉ model h·ªçc
2. **Domain-Adaptive Pre-training**: Chuy√™n m√¥n h√≥a model cho ph√°p lu·∫≠t
3. **Ensemble Learning**: K·∫øt h·ª£p nhi·ªÅu √Ω ki·∫øn chuy√™n gia

#### **ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng:**
1. **Validation**: Ki·ªÉm tra k·∫øt qu·∫£ ·ªü m·ªói t·∫ßng
2. **Logging**: Ghi l·∫°i to√†n b·ªô qu√° tr√¨nh ƒë·ªÉ debug
3. **Monitoring**: Theo d√µi hi·ªáu su·∫•t real-time

## üìä **HI·ªÜU SU·∫§T D·ª∞ KI·∫æN**

V·ªõi ki·∫øn tr√∫c 3 t·∫ßng v√† c√°c k·ªπ thu·∫≠t t·ªëi ∆∞u, h·ªá th·ªëng ƒë·∫°t ƒë∆∞·ª£c s·ª± c√¢n b·∫±ng ·∫•n t∆∞·ª£ng:

### **üìà ƒê·ªô ch√≠nh x√°c theo t·ª´ng t·∫ßng:**

| Metric | T·∫ßng 1: Retrieval | T·∫ßng 2: Light Reranking | T·∫ßng 3: Strong Reranking |
|--------|-------------------|-------------------------|---------------------------|
| **Precision@5** | ~70% | ~80% | **> 90%** |
| **Recall@5** | ~60% | ~75% | **> 85%** |
| **MRR** | ~0.7 | ~0.8 | **> 0.85** |

### **‚ö° Th·ªùi gian x·ª≠ l√Ω:**

| T√°c v·ª• | Th·ªùi gian | M√¥ t·∫£ |
|--------|-----------|-------|
| **T·∫ßng 1**: Retrieval (500 ·ª©ng vi√™n) | ~100ms | T√¨m ki·∫øm r·ªông trong to√†n b·ªô kho d·ªØ li·ªáu |
| **T·∫ßng 2**: Light Reranking (50 ·ª©ng vi√™n) | ~150ms | L·ªçc nhanh v·ªõi Light Reranker |
| **T·∫ßng 3**: Strong Reranking (5 k·∫øt qu·∫£) | ~300ms | Th·∫©m ƒë·ªãnh chuy√™n s√¢u v·ªõi Ensemble |
| **üìä T·ªïng th·ªùi gian ph·∫£n h·ªìi** | **~550ms** | **Nhanh h∆°n 10x so v·ªõi t√¨m ki·∫øm th·ªß c√¥ng** |

### **üéØ So s√°nh v·ªõi ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng:**

| Ti√™u ch√≠ | T√¨m ki·∫øm th·ªß c√¥ng | LawBot v7.0 |
|----------|-------------------|-------------|
| **Th·ªùi gian** | 2-3 gi·ªù | **30 gi√¢y** |
| **ƒê·ªô ch√≠nh x√°c** | 60-70% | **90%+** |
| **Kh·∫£ nƒÉng m·ªü r·ªông** | H·∫°n ch·∫ø | **Kh√¥ng gi·ªõi h·∫°n** |
| **Chi ph√≠** | Cao (nh√¢n l·ª±c) | **Th·∫•p** |



## üõ†Ô∏è **PH√ÅT TRI·ªÇN & B·∫¢O TR√å**

### **Ki·ªÉm tra c·∫•u tr√∫c d·ª± √°n:**

```bash
# Ki·ªÉm tra c·∫•u tr√∫c v√† best practices
python scripts/utils/check_project.py
```

### **L·ªçc d·ªØ li·ªáu (n·∫øu c·∫ßn):**

```bash
# L·ªçc d·ªØ li·ªáu th√¥ ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng
python scripts/utils/run_filter.py
```

### **Ch·∫°y t·ª´ng b∆∞·ªõc ri√™ng l·∫ª:**

```bash
# B∆∞·ªõc 0: DAPT (Domain-Adaptive Pre-training)
python scripts/00_adapt_model.py

# B∆∞·ªõc 1: Ki·ªÉm tra m√¥i tr∆∞·ªùng v√† x·ª≠ l√Ω d·ªØ li·ªáu
python scripts/01_check_environment.py

# B∆∞·ªõc 2: Chu·∫©n b·ªã d·ªØ li·ªáu training
python scripts/02_prepare_training_data.py

# B∆∞·ªõc 3: Hu·∫•n luy·ªán models v√† ƒë√°nh gi√°
python scripts/03_train_models.py
```

## üìã **H∆Ø·ªöNG D·∫™N V·∫¨N H√ÄNH**

### **C√°c t√°c v·ª• h√†ng ng√†y:**

#### **1. Gi√°m s√°t h·ªá th·ªëng**
```bash
# Ki·ªÉm tra logs
tail -f logs/pipeline.log

# Ki·ªÉm tra hi·ªáu su·∫•t
python scripts/utils/check_performance.py

# Ki·ªÉm tra dung l∆∞·ª£ng ·ªï c·ª©ng
df -h
```

#### **2. Sao l∆∞u d·ªØ li·ªáu**
```bash
# Sao l∆∞u models
tar -czf models_backup_$(date +%Y%m%d).tar.gz models/

# Sao l∆∞u d·ªØ li·ªáu
tar -czf data_backup_$(date +%Y%m%d).tar.gz data/

# Sao l∆∞u indexes
tar -czf indexes_backup_$(date +%Y%m%d).tar.gz indexes/
```

#### **3. B·∫£o tr√¨ h·ªá th·ªëng**
```bash
# D·ªçn d·∫πp logs c≈©
find logs/ -name "*.log" -mtime +30 -delete

# D·ªçn d·∫πp checkpoints c≈©
find checkpoints/ -name "*.pt" -mtime +7 -delete

# C·∫≠p nh·∫≠t dependencies
pip install -r requirements.txt --upgrade
```

### **Danh s√°ch tri·ªÉn khai:**

#### **1. Tr∆∞·ªõc khi tri·ªÉn khai**
- [ ] T·∫•t c·∫£ tests ƒë·ªÅu pass
- [ ] Code ƒë√£ ƒë∆∞·ª£c review v√† approve
- [ ] Documentation ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t
- [ ] Performance benchmarks ƒë·∫°t y√™u c·∫ßu
- [ ] Security scan ho√†n t·∫•t

#### **2. Tri·ªÉn khai**
- [ ] Sao l∆∞u phi√™n b·∫£n hi·ªán t·∫°i
- [ ] Tri·ªÉn khai phi√™n b·∫£n m·ªõi
- [ ] Ch·∫°y health checks
- [ ] Gi√°m s√°t l·ªói
- [ ] X√°c minh ch·ª©c nƒÉng

#### **3. Sau khi tri·ªÉn khai**
- [ ] Gi√°m s√°t performance metrics
- [ ] Ki·ªÉm tra feedback ng∆∞·ªùi d√πng
- [ ] C·∫≠p nh·∫≠t monitoring alerts
- [ ] Ghi l·∫°i c√°c v·∫•n ƒë·ªÅ

### **T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t (v8.0):**

#### **1. T·ªëi ∆∞u Model Parameters**
```python
# Bi-Encoder Optimizations (v8.0)
config.BI_ENCODER_BATCH_SIZE = 16                    # TƒÉng t·ª´ 4 l√™n 16
config.BI_ENCODER_EPOCHS = 3                         # TƒÉng t·ª´ 1 l√™n 3
config.BI_ENCODER_LR = 2e-5                          # TƒÉng t·ª´ 1e-7 l√™n 2e-5
config.BI_ENCODER_WARMUP_STEPS = 100                 # TƒÉng t·ª´ 50 l√™n 100
config.BI_ENCODER_EVAL_STEPS = 50                    # TƒÉng t·ª´ 25 l√™n 50
config.BI_ENCODER_GRADIENT_ACCUMULATION_STEPS = 2    # M·ªõi th√™m

# Cross-Encoder Optimizations (v8.0)
config.CROSS_ENCODER_BATCH_SIZE = 8                  # TƒÉng t·ª´ 4 l√™n 8
config.CROSS_ENCODER_EPOCHS = 5                      # TƒÉng t·ª´ 1 l√™n 5
config.CROSS_ENCODER_LR = 2e-5                       # TƒÉng t·ª´ 5e-6 l√™n 2e-5
config.CROSS_ENCODER_WARMUP_STEPS = 100              # TƒÉng t·ª´ 25 l√™n 100
config.CROSS_ENCODER_EVAL_STEPS = 100                # TƒÉng t·ª´ 50 l√™n 100
config.CROSS_ENCODER_GRADIENT_ACCUMULATION_STEPS = 4 # M·ªõi th√™m
```

#### **2. T·ªëi ∆∞u Operational Performance (v8.0)**
```python
# Mixed Precision Training (FP16)
config.FP16_TRAINING = True                           # TƒÉng t·ªëc ƒë·ªô 1.5-2x, gi·∫£m 50% VRAM

# DataLoader Optimizations
config.BI_ENCODER_DATALOADER_NUM_WORKERS = 4         # TƒÉng t·ª´ 1 l√™n 4
config.CROSS_ENCODER_DATALOADER_NUM_WORKERS = 4      # TƒÉng t·ª´ 1 l√™n 4
config.BI_ENCODER_DATALOADER_PIN_MEMORY = True       # T·ªëi ∆∞u GPU transfer
config.CROSS_ENCODER_DATALOADER_PIN_MEMORY = True    # T·ªëi ∆∞u GPU transfer
config.BI_ENCODER_DATALOADER_PREFETCH_FACTOR = 2     # M·ªõi th√™m
config.CROSS_ENCODER_DATALOADER_PREFETCH_FACTOR = 2  # M·ªõi th√™m
```

#### **2. T·ªëi ∆∞u h·ªá th·ªëng**
```bash
# TƒÉng file descriptors
ulimit -n 65536

# T·ªëi ∆∞u memory usage
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# B·∫≠t mixed precision
export CUDA_LAUNCH_BLOCKING=1
```

#### **3. T·ªëi ∆∞u FAISS Index**
```python
# FAISS index optimization
faiss.omp_set_num_threads(8)  # Set s·ªë threads
index.nprobe = 64             # ƒêi·ªÅu ch·ªânh search parameters
```

### **Ki·ªÉm tra c·∫•u tr√∫c d·ª± √°n:**

```bash
# Ki·ªÉm tra c·∫•u tr√∫c project v√† best practices
python scripts/utils/check_project.py
```

## üìö **API DOCUMENTATION**

### **LegalQAPipeline**

Class ch√≠nh ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi h·ªá th·ªëng.

```python
from core.pipeline import LegalQAPipeline

# Kh·ªüi t·∫°o
pipeline = LegalQAPipeline()

# Ki·ªÉm tra tr·∫°ng th√°i
if pipeline.is_ready:
    print("Pipeline s·∫µn s√†ng!")
else:
    print("Pipeline ch∆∞a s·∫µn s√†ng!")
```

#### **Methods:**

##### `predict(query, top_k_retrieval=100, top_k_final=5)`

D·ª± ƒëo√°n c√¢u tr·∫£ l·ªùi cho c√¢u h·ªèi.

**Parameters:**
- `query` (str): C√¢u h·ªèi c·∫ßn tr·∫£ l·ªùi
- `top_k_retrieval` (int): S·ªë l∆∞·ª£ng k·∫øt qu·∫£ retrieval (default: 100)
- `top_k_final` (int): S·ªë l∆∞·ª£ng k·∫øt qu·∫£ cu·ªëi c√πng (default: 5)

**Returns:**
- `List[Dict]`: Danh s√°ch k·∫øt qu·∫£ v·ªõi format:
  ```python
  [
      {
          "aid": "law_1_113",
          "content": "ƒêi·ªÅu 113: Ng∆∞·ªùi lao ƒë·ªông ƒë∆∞·ª£c ngh·ªâ ph√©p nƒÉm...",
          "retrieval_score": 0.85,
          "rerank_score": 0.92
      },
      # ...
  ]
  ```

**V√≠ d·ª• s·ª≠ d·ª•ng:**
```python
# Input
query = "Ng∆∞·ªùi lao ƒë·ªông ƒë∆∞·ª£c ngh·ªâ ph√©p bao nhi√™u ng√†y?"

# Output
results = [
    {
        "aid": "law_1_113",
        "content": "ƒêi·ªÅu 113. Ng∆∞·ªùi lao ƒë·ªông ƒë∆∞·ª£c ngh·ªâ ph√©p nƒÉm 12 ng√†y l√†m vi·ªác...",
        "retrieval_score": 0.85,
        "rerank_score": 0.92
    },
    {
        "aid": "law_1_114",
        "content": "ƒêi·ªÅu 114. Th·ªùi gian ngh·ªâ ph√©p nƒÉm ƒë∆∞·ª£c t√≠nh theo nƒÉm l√†m vi·ªác...",
        "retrieval_score": 0.78,
        "rerank_score": 0.87
    }
]
```

##### `retrieve(query, top_k=100)`

Ch·ªâ th·ª±c hi·ªán retrieval (t·∫ßng 1).

**Parameters:**
- `query` (str): C√¢u h·ªèi
- `top_k` (int): S·ªë l∆∞·ª£ng k·∫øt qu·∫£

**Returns:**
- `Tuple[List[str], List[float]]`: (aids, scores)

**V√≠ d·ª•:**
```python
# Input
query = "ƒêi·ªÅu ki·ªán th√†nh l·∫≠p doanh nghi·ªáp?"

# Output
aids = ["law_2_15", "law_2_16", "law_2_17", ...]
scores = [0.95, 0.87, 0.82, ...]
```

##### `rerank(query, retrieved_aids, retrieved_distances)`

Ch·ªâ th·ª±c hi·ªán reranking (t·∫ßng 2).

**Parameters:**
- `query` (str): C√¢u h·ªèi
- `retrieved_aids` (List[str]): Danh s√°ch AIDs t·ª´ retrieval
- `retrieved_distances` (List[float]): ƒêi·ªÉm s·ªë t·ª´ retrieval

**Returns:**
- `List[Dict]`: K·∫øt qu·∫£ ƒë√£ rerank

**V√≠ d·ª•:**
```python
# Input
query = "Ng∆∞·ªùi lao ƒë·ªông ƒë∆∞·ª£c ngh·ªâ ph√©p bao nhi√™u ng√†y?"
retrieved_aids = ["law_1_113", "law_1_114", "law_1_115"]
retrieved_distances = [0.85, 0.78, 0.72]

# Output
reranked_results = [
    {
        "aid": "law_1_113",
        "content": "ƒêi·ªÅu 113. Ng∆∞·ªùi lao ƒë·ªông ƒë∆∞·ª£c ngh·ªâ ph√©p nƒÉm...",
        "retrieval_score": 0.85,
        "rerank_score": 0.92
    },
    {
        "aid": "law_1_114", 
        "content": "ƒêi·ªÅu 114. Th·ªùi gian ngh·ªâ ph√©p nƒÉm...",
        "retrieval_score": 0.78,
        "rerank_score": 0.87
    }
]
```

## üõ†Ô∏è **UTILITIES**

### **Dataset Filtering Utility**

Script ƒë·ªÉ l·ªçc dataset tr∆∞·ªõc khi ch·∫°y pipeline ch√≠nh:

```bash
# Ch·∫°y filtering utility
python scripts/utils/run_filter.py

# Ho·∫∑c ch·∫°y tr·ª±c ti·∫øp
python scripts/utils/filter_dataset.py
```

**Ch·ª©c nƒÉng:**
- L·ªçc b·ªè samples c√≥ ground truth kh√¥ng ph√π h·ª£p
- Gi·ªØ l·∫°i ~100-200 samples ch·∫•t l∆∞·ª£ng cao
- C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu training

### **Project Structure Checker**

Ki·ªÉm tra c·∫•u tr√∫c project v√† best practices:

```bash
python scripts/utils/check_project.py
```

**Ch·ª©c nƒÉng:**
- Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c
- Validate naming conventions
- Ki·ªÉm tra documentation
- ƒê·∫£m b·∫£o best practices

## üõ†Ô∏è **DEVELOPMENT & BEST PRACTICES**

### **Project Structure Check:**

```bash
# Ki·ªÉm tra c·∫•u tr√∫c project v√† best practices
python scripts/utils/check_project.py
```

### **Code Quality Standards:**

```bash
# Format code
black .

# Lint code
flake8 .

# Type checking
mypy .

# Run tests
pytest tests/
```

### **Best Practices Applied:**

#### **1. Separation of Concerns**
- **Pipeline Scripts**: M·ªói script c√≥ m·ªôt nhi·ªám v·ª• c·ª• th·ªÉ
- **Utility Scripts**: T√°ch ri√™ng v√†o th∆∞ m·ª•c `scripts/utils/`
- **Core Modules**: T√°ch bi·ªát logic nghi·ªáp v·ª• v√† infrastructure

#### **2. Naming Conventions**
- **Pipeline Steps**: `01_`, `02_`, `03_` prefix cho c√°c b∆∞·ªõc ch√≠nh
- **Utility Scripts**: T√™n m√¥ t·∫£ r√µ ch·ª©c nƒÉng
- **Functions**: snake_case cho Python functions
- **Classes**: PascalCase cho class names

#### **3. Error Handling**
- **Graceful Degradation**: H·ªá th·ªëng v·∫´n ho·∫°t ƒë·ªông khi c√≥ l·ªói
- **Detailed Logging**: Log ƒë·∫ßy ƒë·ªß th√¥ng tin l·ªói
- **User-Friendly Messages**: Th√¥ng b√°o l·ªói d·ªÖ hi·ªÉu

#### **4. Configuration Management**
- **Environment Variables**: H·ªó tr·ª£ c·∫•u h√¨nh qua env vars
- **Centralized Config**: T·∫•t c·∫£ config trong `config.py`
- **Validation**: Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa config

### **Performance Optimization:**

#### **1. Memory Management**
- **Batch Processing**: X·ª≠ l√Ω theo batch ƒë·ªÉ ti·∫øt ki·ªám memory
- **GPU Utilization**: T·ªëi ∆∞u s·ª≠ d·ª•ng GPU
- **Model Caching**: Cache models ƒë·ªÉ tr√°nh load l·∫°i

#### **2. Speed Optimization**
- **FAISS Index**: S·ª≠ d·ª•ng FAISS cho retrieval nhanh
- **Parallel Processing**: X·ª≠ l√Ω song song khi c√≥ th·ªÉ
- **Efficient Data Structures**: S·ª≠ d·ª•ng c·∫•u tr√∫c d·ªØ li·ªáu hi·ªáu qu·∫£

## üîÑ **TECHNICAL ARCHITECTURE DETAILS**

### **Core Components:**

#### **1. Bi-Encoder (Retrieval Layer)**
```python
# Model: bkai-foundation-models/vietnamese-bi-encoder
# Purpose: Encode questions and documents into vectors
# Output: 768-dimensional embeddings
# Usage: FAISS similarity search

class BiEncoderComponent:
    def encode(self, texts: List[str]) -> np.ndarray:
        """Encode texts to embeddings"""
        embeddings = self.model.encode(texts, convert_to_tensor=True)
        return embeddings.cpu().numpy()
    
    def search(self, query_embedding: np.ndarray, top_k: int) -> Tuple[List[int], List[float]]:
        """Search similar documents using FAISS"""
        distances, indices = self.faiss_index.search(query_embedding, top_k)
        return indices, distances
```

#### **2. Cross-Encoder (Reranking Layer)**
```python
# Model: vinai/phobert-large
# Purpose: Score question-document pairs
# Input: [CLS] question [SEP] document [SEP]
# Output: Binary classification score (0-1)

class CrossEncoderComponent:
    def score_pairs(self, pairs: List[List[str]]) -> List[float]:
        """Score question-document pairs"""
        tokenized = self.tokenizer(
            pairs,
            padding=True,
            truncation=True,
            max_length=256,
            return_tensors="pt"
        )
        
        with torch.no_grad():
            logits = self.model(**tokenized).logits
            scores = torch.softmax(logits, dim=1)[:, 1].cpu().tolist()
        
        return scores
```

#### **3. FAISS Index**
```python
# Type: IndexFlatIP (Inner Product)
# Dimension: 768
# Normalization: L2 normalization
# Size: ~100MB for 100K documents

class FAISSIndex:
    def __init__(self, dimension: int = 768):
        self.index = faiss.IndexFlatIP(dimension)
        self.index_to_aid = {}
    
    def add_documents(self, embeddings: np.ndarray, aids: List[str]):
        """Add document embeddings to index"""
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        
        # Map index positions to AIDs
        start_idx = len(self.index_to_aid)
        for i, aid in enumerate(aids):
            self.index_to_aid[start_idx + i] = aid
```

### **Data Flow Architecture:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Raw Data      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Preprocessing  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Training Data  ‚îÇ
‚îÇ   (JSON files)  ‚îÇ    ‚îÇ  Pipeline       ‚îÇ    ‚îÇ  (Triplets/Pairs)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                       ‚îÇ
                                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Models        ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Training       ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Model Config   ‚îÇ
‚îÇ   (Saved)       ‚îÇ    ‚îÇ  Pipeline       ‚îÇ    ‚îÇ  (Hyperparams)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                       ‚îÇ
                                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   FAISS Index   ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Index Building ‚îÇ‚óÄ‚îÄ‚îÄ‚îÄ‚îÇ  Document       ‚îÇ
‚îÇ   (Binary)      ‚îÇ    ‚îÇ  Pipeline       ‚îÇ    ‚îÇ  Embeddings     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Request Processing Pipeline:**

#### **1. Input Validation**
```python
def validate_query(query: str) -> bool:
    """Validate input query"""
    if not query or len(query.strip()) == 0:
        return False
    if len(query) > 1000:  # Max length
        return False
    return True
```

#### **2. Retrieval Phase**
```python
def retrieval_phase(query: str, top_k: int = 100) -> Tuple[List[str], List[float]]:
    """Phase 1: Retrieve relevant documents"""
    # 1. Encode query
    query_embedding = bi_encoder.encode([query])
    
    # 2. Search FAISS index
    distances, indices = faiss_index.search(query_embedding, top_k)
    
    # 3. Convert to AIDs
    retrieved_aids = [index_to_aid[i] for i in indices[0]]
    
    return retrieved_aids, distances[0]
```

#### **3. Reranking Phase**
```python
def reranking_phase(query: str, retrieved_aids: List[str]) -> List[Dict]:
    """Phase 2: Rerank retrieved documents"""
    # 1. Get document contents
    documents = [aid_map[aid] for aid in retrieved_aids]
    
    # 2. Create pairs
    pairs = [[query, doc] for doc in documents]
    
    # 3. Score pairs
    scores = cross_encoder.score_pairs(pairs)
    
    # 4. Create results
    results = [
        {
            "aid": aid,
            "content": doc,
            "rerank_score": score
        }
        for aid, doc, score in zip(retrieved_aids, documents, scores)
    ]
    
    return sorted(results, key=lambda x: x["rerank_score"], reverse=True)
```

### **Performance Metrics:**

#### **1. Retrieval Metrics**
- **Precision@K**: T·ª∑ l·ªá documents li√™n quan trong top-K
- **Recall@K**: T·ª∑ l·ªá documents li√™n quan ƒë∆∞·ª£c t√¨m th·∫•y
- **F1@K**: Harmonic mean c·ªßa Precision v√† Recall
- **MRR**: Mean Reciprocal Rank

#### **2. Reranking Metrics**
- **Accuracy**: ƒê·ªô ch√≠nh x√°c c·ªßa binary classification
- **AUC-ROC**: Area under ROC curve
- **Precision-Recall**: Trade-off gi·ªØa precision v√† recall

#### **3. End-to-End Metrics**
- **Response Time**: Th·ªùi gian x·ª≠ l√Ω t·ª´ request ƒë·∫øn response
- **Throughput**: S·ªë requests x·ª≠ l√Ω ƒë∆∞·ª£c trong 1 gi√¢y
- **Memory Usage**: L∆∞·ª£ng memory s·ª≠ d·ª•ng

## üö® **TROUBLESHOOTING & BEST PRACTICES (v8.1)**

### **üîß Troubleshooting th∆∞·ªùng g·∫∑p:**

#### **1. GPU Issues**
```bash
# Ki·ªÉm tra GPU availability
python -c "import torch; print(f'GPU: {torch.cuda.is_available()}')"

# Force CPU training n·∫øu c·∫ßn
export CUDA_VISIBLE_DEVICES=""
python run_pipeline.py
```

#### **2. Memory Issues**
```bash
# Gi·∫£m batch size
export LAWBOT_BI_ENCODER_BATCH_SIZE=8
export LAWBOT_CROSS_ENCODER_BATCH_SIZE=4

# T·∫Øt mixed precision
export LAWBOT_FP16_TRAINING=false
```

#### **3. Model Loading Issues**
```bash
# Ki·ªÉm tra model paths
ls -la models/
ls -la models/phobert-law/
ls -la models/bi-encoder/

# Rebuild models n·∫øu c·∫ßn
python scripts/00_adapt_model.py
python scripts/03_train_models.py
```

#### **4. Data Loading Issues**
```bash
# Ki·ªÉm tra data files
ls -la data/raw/
ls -la data/processed/

# Validate data structure
python scripts/utils/check_project.py
```

### **‚ö° Performance Optimization Tips:**

#### **1. GPU Optimization**
```bash
# T·ªëi ∆∞u GPU memory
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# B·∫≠t mixed precision
export LAWBOT_FP16_TRAINING=true
```

#### **2. System Optimization**
```bash
# TƒÉng file descriptors
ulimit -n 65536

# T·ªëi ∆∞u CPU cores
export OMP_NUM_THREADS=8
```

#### **3. Pipeline Optimization**
```bash
# Skip DAPT n·∫øu kh√¥ng c·∫ßn
python run_pipeline.py --no-dapt

# Ch·∫°y t·ª´ b∆∞·ªõc c·ª• th·ªÉ
python run_pipeline.py --step 02
```

### **üìä Monitoring & Logging:**

#### **1. Performance Monitoring**
```bash
# Monitor GPU usage
nvidia-smi -l 1

# Monitor memory usage
htop

# Check logs
tail -f logs/pipeline.log
```

#### **2. Quality Metrics**
```bash
# Check evaluation results
ls -la reports/

# View latest report
cat reports/evaluation_report_*.json
```

## üö® **TROUBLESHOOTING**

Tham kh·∫£o c√°c l·ªói th∆∞·ªùng g·∫∑p v√† c√°ch kh·∫Øc ph·ª•c chi ti·∫øt trong file `QUICK_START.md` ho·∫∑c `DEPLOYMENT_GUIDE.md`.

## üîÑ **MIGRATION GUIDE (v8.0 ‚Üí v8.1)**

### **üöÄ Nh·ªØng thay ƒë·ªïi ch√≠nh:**

#### **1. Breaking Changes**
- **Kh√¥ng c√≥ breaking changes**: T·∫•t c·∫£ APIs v√† configs v·∫´n t∆∞∆°ng th√≠ch
- **Backward compatibility**: Code c≈© v·∫´n ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng
- **Auto-detection**: H·ªá th·ªëng t·ª± ƒë·ªông detect v√† t·ªëi ∆∞u h√≥a

#### **2. Performance Improvements**
- **GPU acceleration**: T·ª± ƒë·ªông s·ª≠ d·ª•ng GPU n·∫øu c√≥
- **Model reuse**: T√°i s·ª≠ d·ª•ng model c√≥ s·∫µn thay v√¨ train t·∫°m
- **Single data loading**: Gi·∫£m th·ªùi gian load d·ªØ li·ªáu
- **Memory optimization**: T·ªëi ∆∞u h√≥a memory usage

#### **3. New Features**
- **Enhanced logging**: Log chi ti·∫øt h∆°n v·ªõi performance metrics
- **Better error handling**: Graceful degradation v√† fallback
- **Auto-validation**: T·ª± ƒë·ªông validate d·ªØ li·ªáu v√† model
- **Performance monitoring**: Theo d√µi hi·ªáu su·∫•t real-time

### **üìã Migration Steps:**

#### **Step 1: Backup**
```bash
# Backup current version
cp -r models/ models_backup_v8.0/
cp -r data/processed/ data_backup_v8.0/
```

#### **Step 2: Update Code**
```bash
# Pull latest changes
git pull origin main

# Update dependencies
pip install -r requirements.txt --upgrade
```

#### **Step 3: Test**
```bash
# Test GPU detection
python -c "import torch; print(f'GPU: {torch.cuda.is_available()}')"

# Test model loading
python scripts/utils/check_project.py
```

#### **Step 4: Run Pipeline**
```bash
# Run full pipeline with optimizations
python run_pipeline.py

# Or run specific steps
python run_pipeline.py --step 02
```

### **üéØ Expected Improvements:**

| Metric | v8.0 | v8.1 | Improvement |
|--------|------|------|-------------|
| **Training Time** | 2-3 hours | 30-60 minutes | **3-6x faster** |
| **Memory Usage** | High | Optimized | **50% reduction** |
| **Error Recovery** | Manual | Automatic | **100% reliability** |
| **GPU Utilization** | Manual | Auto-detect | **Seamless** |

## üìñ **T√ÄI LI·ªÜU THAM KH·∫¢O**

### **Papers:**
- [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
- [Cross-Encoders vs. Bi-Encoders for Zero-Shot Classification](https://arxiv.org/abs/2108.08877)
- [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906)

### **Libraries:**
- [Sentence Transformers](https://www.sbert.net/)
- [FAISS](https://github.com/facebookresearch/faiss)
- [Transformers](https://huggingface.co/transformers/)
- [PyTorch](https://pytorch.org/)

### **Datasets:**
- [Vietnamese Legal Corpus](https://github.com/lawbot-team/vietnamese-legal-corpus)
- [Legal QA Dataset](https://github.com/lawbot-team/legal-qa-dataset)

## ü§ù **ƒê√ìNG G√ìP**

Ch√∫ng t√¥i r·∫•t hoan ngh√™nh m·ªçi ƒë√≥ng g√≥p! Vui l√≤ng ƒë·ªçc `CONTRIBUTING.md` ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt.

## üìÑ **LICENSE**

D·ª± √°n n√†y ƒë∆∞·ª£c c·∫•p ph√©p theo MIT License.

---

**Made with ‚ù§Ô∏è by LawBot Team**
